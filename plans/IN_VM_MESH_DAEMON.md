# In-VM Mesh Daemon

> **Context:** VMs run omertad inside the guest and join the mesh as
> independent peers. The provider host does not proxy VM packets — the VM
> handles its own mesh participation. This plan covers getting omertad
> installed and running inside VMs so they can join the virtual network.
>
> **Leads into:** VIRTUAL_NETWORK_REWORK.md Phase 15 (VM Network Integration),
> which assumes the daemon is already running in the VM and tests SSH, ping,
> and multi-VM communication.
>
> **Replaces:** TUNNEL_INFRASTRUCTURE.md Phase 1 (VMPacketCapture). The
> external packet capture approach is superseded by this in-VM daemon model.
>
> **Related plans:**
> - VIRTUAL_NETWORK_REWORK.md — virtual network architecture, phases 1-16
> - TUNNEL_INFRASTRUCTURE.md — health monitoring, failure handling, cleanup
> - GOSSIP_RELAY_PLAN.md — relay discovery gossip

## Overview

Each VM is its **own peer** on the cloister network. The consumer generates
an `IdentityKeypair` for the VM and sends it over the cloister. The VM
boots with that identity and bootstraps through the **consumer**, which is
the VM's bootstrap node. Because the VM initiates contact with the consumer,
the consumer hears from the VM directly and can detect it by its PeerId
(which the consumer knows because it generated the keypair).

The VM request is **multi-part**:

1. **Request** (main mesh) — consumer asks the provider for a VM, sending
   only non-sensitive data (resources, SSH keys).
2. **Acceptance** (main mesh) — provider agrees and responds with a vmId.
3. **Cloister creation** — consumer negotiates a cloister (private encrypted
   overlay) with the provider.
4. **Provisioning data** (cloister only) — consumer sends the VM's
   generated `IdentityKeypair`, subnet, and cloister network key.
5. **VM launch** — provider builds and boots the VM.
6. **VM bootstraps to consumer** — VM contacts the consumer to join the
   cloister network. Consumer detects the VM via `ping(vmPeerId)`.

```
Consumer                              Provider
   │                                     │
   │  1. VM Request (main mesh)          │
   │  { sshKeys, resources }       ──→   │
   │                                     │
   │  2. VM Accepted (main mesh)         │
   │  { vmId }                     ←──   │
   │                                     │
   │  3. Cloister negotiation            │
   │  (X25519 key exchange               │
   │   over main mesh)                   │
   │  ←─────────────────────────────→    │
   │                                     │
   │  4. Provisioning data (cloister)    │
   │  { vmIdentity, subnet,       ──→   │
   │    cloisterKey }                    │
   │                                     │
   │  5. Provider launches VM            │
   │                                     │
   │            ┌─────────┐              │
   │            │   VM    │              │
   │            │ (own    │              │
   │            │  peer)  │              │
   │            └────┬────┘              │
   │                 │                   │
   │  6. VM bootstraps to consumer       │
   │  ←──────────────┘                   │
   │                                     │
   │  Consumer detects VM:               │
   │  ping(vmPeerId) succeeds            │
```

```
┌──────────────────────────────────────────────────────────┐
│  VM Guest                                                │
│  ├── PeerId: "vm-peer-id" (its own, generated by consumer)│
│  ├── MachineId: auto-generated on boot                   │
│                                                          │
│  ┌──────────┐    ┌───────────────────────────────────┐   │
│  │ sshd,    │    │ omertad (uploaded by provider)    │   │
│  │ apps     │    │ ├── MeshNetwork (VM's own identity)│   │
│  │          │    │ ├── TUN or netstack interface      │   │
│  └────┬─────┘    │ └── Bootstraps to consumer        │   │
│       │          └──────────┬────────────────────────┘   │
│       └─────────────────────┘                            │
│              (omerta0 interface or netstack)              │
└──────────────────────────────────────────────────────────┘
          │ (mesh UDP via eth0 to provider,
          │  then to consumer as bootstrap)
          │
          │ All application traffic routes through omerta0:
          │   VM → omerta0 → mesh → consumer (gateway) → internet
          │
          │ eth0 carries ONLY encrypted mesh UDP.
          │ The VM has no direct internet access.
          ▼
    Provider's physical network → consumer (bootstrap peer)
```

**Key design decisions:**
- **VM is its own peer.** The consumer generates an `IdentityKeypair` for
  the VM. The VM has its own `PeerId`, distinct from the consumer's.
- **Consumer is the bootstrap node.** The VM contacts the consumer to join
  the cloister network. The consumer hears from the VM directly.
- **Multi-part request.** The initial request carries only non-sensitive
  data on the main mesh. Sensitive provisioning data (VM identity, subnet,
  cloister key) is sent only after the provider accepts and a cloister is
  established.
- **Cloister network.** The VM's private key never travels over the public
  mesh. A cloister is negotiated between consumer and provider after
  acceptance.
- **Binary upload.** The provider writes the omertad binary to the VM disk
  directly. The VM needs no internet access.
- **Consumer as gateway.** All internet-bound traffic from the VM routes
  through the mesh overlay to the consumer, which is the gateway on the
  virtual network.
- **Stock cloud images.** No custom image building. Cloud-init handles all
  configuration.
- **Consumer-provided subnet.** The consumer knows its subnet and passes it
  in the provisioning data over the cloister.
- **Standard ping.** Since the VM is its own peer, regular `ping(vmPeerId)`
  works. No machine-level ping overload needed.

---

## Phase 1: Machine-Level Ping

**Goal:** Overload `ping` to accept a `MachineId` so that individual
machines under the same peer can be pinged. While the VM use case doesn't
require this (VMs are their own peers), this is useful for any scenario
where multiple machines share a `PeerId` (e.g. same user on laptop +
desktop).

Currently `sendPingWithDetails(to: PeerId)` resolves endpoints by PeerId
via `endpointManager.getAllEndpoints(peerId:)` and picks the first one.
When multiple machines share a PeerId, this is ambiguous — you might hit
the wrong machine.

### Changes Required

**`MeshNode` — add machine-targeted ping:**

```swift
/// Ping a specific machine. Used when multiple machines share a PeerId.
public func sendPingWithDetails(
    toMachine machineId: MachineId,
    timeout: TimeInterval = 3.0,
    requestFullList: Bool = false
) async -> PingResult? {
    // Look up peerId from registry
    guard let peerId = await machinePeerRegistry.getMostRecentPeer(for: machineId) else {
        return nil
    }

    // Get endpoints for this specific machine (not all machines for the peer)
    let endpoints = await endpointManager.getEndpoints(peerId: peerId, machineId: machineId)
    guard let endpoint = EndpointUtils.preferredEndpoint(from: endpoints) else {
        return nil
    }

    // Same ping logic as the PeerId variant, but targeted to one machine
    // ...
}
```

**`MeshNetwork` — overloaded public API:**

Since `PeerId` and `MachineId` are both `String` typealiases, the
overloads need labeled arguments to disambiguate:

```swift
/// Ping a peer. Assumes the peer has a single machine.
/// If the peer has multiple machines, the target is ambiguous.
public func ping(
    _ targetPeerId: PeerId,
    timeout: TimeInterval = 3.0,
    requestFullList: Bool = false
) async -> MeshNode.PingResult?

/// Ping a specific machine. Use this when the peer has multiple machines
/// (e.g. same user on laptop + desktop).
public func ping(
    machineId: MachineId,
    timeout: TimeInterval = 3.0,
    requestFullList: Bool = false
) async -> MeshNode.PingResult?
```

The existing `ping(_ targetPeerId:)` keeps its current behavior (first
endpoint found for that PeerId). The new `ping(machineId:)` variant
looks up the specific machine's endpoints via
`endpointManager.getEndpoints(peerId:machineId:)`.

### Files to Modify

| File | Changes |
|------|---------|
| `Sources/OmertaMesh/MeshNode.swift` | Add `sendPingWithDetails(toMachine:)` |
| `Sources/OmertaMesh/Public/MeshNetwork.swift` | Add `ping(machineId:)` overload |

### Unit Tests

| Test | Description |
|------|-------------|
| `testPingByMachineId` | Ping a specific machine, verify response |
| `testPingByMachineIdWhenMultipleMachinesSamePeer` | Two machines with same PeerId, ping each individually |
| `testPingByMachineIdUnknownMachine` | Unknown MachineId returns nil |
| `testPingByPeerIdStillWorks` | Existing PeerId ping is unchanged |
| `testPingByMachineIdTimeout` | Machine exists in registry but unreachable, verify timeout/nil |
| `testPingByMachineIdReturnsCorrectLatency` | Verify RTT in PingResult matches the targeted machine |
| `testPingByMachineIdUsesCorrectEndpoint` | Verify the machine-specific endpoint is used, not another machine's |
| `testPingByMachineIdRegistryStale` | Machine in registry but peerId no longer valid, returns nil |
| `testPingByPeerIdWithSingleMachine` | PeerId ping with one machine behaves identically to machineId ping |

### Integration Tests

| Test | Description |
|------|-------------|
| `testPingMachineRoundTrip` | Two nodes sharing a PeerId, ping each by machineId, verify distinct responses |
| `testPingMachineAfterEndpointChange` | Machine changes endpoint, verify ping still reaches it after registry update |

---

## Phase 2: Multi-Part VM Request Protocol

**Goal:** Implement the multi-part VM request protocol. The initial request
happens on the main mesh with only non-sensitive data. After the provider
accepts, the consumer creates a cloister and sends provisioning data
(VM identity, subnet, cloister key) over it.

### Message Types

```swift
/// Step 1: VM request — sent on the main mesh.
/// Contains only non-sensitive data.
public struct VMRequest: Codable, Sendable {
    public let requestId: UUID
    public let sshPublicKeys: [String]
    public let sshUser: String
    public let resources: ResourceRequirements
}

/// Step 2: VM acceptance — sent on the main mesh.
/// Provider agrees to host the VM.
public struct VMAcceptance: Codable, Sendable {
    public let requestId: UUID
    public let vmId: UUID
}

/// Step 2 (alt): VM rejection — sent on the main mesh.
public struct VMRejection: Codable, Sendable {
    public let requestId: UUID
    public let reason: String
}

/// Step 3 is cloister negotiation (CloisterClient.negotiate()).

/// Step 4: VM provisioning data — sent over the cloister only.
/// Contains sensitive data that must not travel on the main mesh.
public struct VMProvisioningData: Codable, Sendable {
    /// The vmId from the acceptance
    public let vmId: UUID

    /// Generated identity for the VM — the VM will be its own peer
    public let vmIdentity: IdentityKeypair

    /// Consumer's subnet for the VM (e.g. "10.0.1.0/24")
    public let subnet: String

    /// Cloister network key so the VM can join the cloister
    public let cloisterNetworkKey: Data
}
```

### Protocol Flow

```
Consumer                              Provider
   │                                     │
   │  "vm-request" (main mesh)           │
   │  VMRequest { requestId,       ──→   │
   │    sshKeys, resources }             │
   │                                     │  Provider checks capacity,
   │                                     │  resources, etc.
   │  "vm-response-{peerId}"             │
   │  (main mesh)                        │
   │  VMAcceptance { requestId,    ←──   │
   │    vmId }                           │
   │                                     │
   │  CloisterClient.negotiate()         │
   │  (X25519 key exchange over          │
   │   main mesh — only public keys      │
   │   are exchanged)                    │
   │  ←─────────────────────────────→    │
   │                                     │
   │  Both derive cloister key           │
   │  (32-byte symmetric key via HKDF)   │
   │                                     │
   │  Consumer generates IdentityKeypair │
   │  for the VM (vm is its own peer)    │
   │                                     │
   │  "vm-provision-{vmId}" (cloister)   │
   │  VMProvisioningData { vmId,   ──→   │
   │    vmIdentity, subnet,              │
   │    cloisterNetworkKey }             │
   │                                     │  Provider now has everything
   │                                     │  needed to launch the VM.
   │                                     │
   │  "vm-provision-ack-{peerId}"        │
   │  (cloister)                         │
   │  { vmId, status: "launching" } ←──  │
   │                                     │
   │  Provider launches VM...            │
   │  VM boots → omertad starts →        │
   │  contacts consumer (bootstrap)      │
   │                                     │
   │  Consumer detects VM:               │
   │  ping(vmPeerId) succeeds            │
```

### Consumer Implementation

```swift
extension MeshConsumerClient {
    public func requestVM(
        requirements: ResourceRequirements = ResourceRequirements(),
        sshPublicKey: String,
        sshKeyPath: String = "~/.omerta/ssh/id_ed25519",
        sshUser: String = "omerta",
        subnet: String,
        timeoutMinutes: Int = 10
    ) async throws -> VMConnection {
        // Step 1: Send request on main mesh (non-sensitive data only)
        let requestId = UUID()
        let request = VMRequest(
            requestId: requestId,
            sshPublicKeys: [sshPublicKey],
            sshUser: sshUser,
            resources: requirements
        )
        try await meshNetwork.sendOnChannel(
            JSONCoding.encoder.encode(request),
            to: providerPeerId,
            channel: "vm-request"
        )

        // Step 2: Wait for acceptance on main mesh
        let acceptance = try await waitForAcceptance(
            requestId: requestId,
            timeout: .seconds(60)
        )

        // Step 3: Create cloister with provider
        let cloister = CloisterClient(provider: meshNetwork)
        let cloisterResult = try await cloister.negotiate(
            with: providerPeerId,
            networkName: "vm-\(acceptance.vmId)"
        )

        // Step 4: Generate identity for the VM and send over cloister
        let (vmKeypair, _) = IdentityKeypair.generate()
        let vmPeerId = vmKeypair.identity.peerId

        let provisioningData = VMProvisioningData(
            vmId: acceptance.vmId,
            vmIdentity: vmKeypair,
            subnet: subnet,
            cloisterNetworkKey: cloisterResult.networkKey
        )
        try await sendOverCloister(
            provisioningData,
            cloisterKey: cloisterResult.networkKey,
            channel: "vm-provision-\(acceptance.vmId)"
        )

        // Step 5: Wait for provisioning ack over cloister
        try await waitForProvisionAck(
            vmId: acceptance.vmId,
            cloisterKey: cloisterResult.networkKey,
            timeout: .seconds(30)
        )

        // Step 6: Wait for VM to bootstrap to us
        // The VM will contact us as its bootstrap peer.
        // We know its PeerId because we generated the keypair.
        let deadline = ContinuousClock.now + .minutes(timeoutMinutes)
        while ContinuousClock.now < deadline {
            if let result = await meshNetwork.ping(vmPeerId) {
                return VMConnection(
                    vmId: acceptance.vmId,
                    provider: PeerInfo(peerId: providerPeerId, endpoint: ""),
                    vmIP: result.meshIP ?? "",
                    sshKeyPath: sshKeyPath,
                    sshUser: sshUser,
                    vpnInterface: "omerta0",
                    createdAt: Date(),
                    networkId: networkId
                )
            }
            try await Task.sleep(for: .seconds(5))
        }

        throw VMError.meshJoinTimeout(vmPeerId: vmPeerId)
    }
}
```

### Provider Implementation

```swift
extension MeshProviderDaemon {
    func handleVMRequest(_ request: VMRequest, from consumerPeerId: PeerId) async throws {
        // Check capacity
        guard activeVMCount < config.maxVMs else {
            try await sendRejection(
                requestId: request.requestId,
                reason: "At capacity",
                to: consumerPeerId
            )
            return
        }

        // Accept — assign a vmId
        let vmId = UUID()
        let acceptance = VMAcceptance(requestId: request.requestId, vmId: vmId)
        try await meshNetwork.sendOnChannel(
            JSONCoding.encoder.encode(acceptance),
            to: consumerPeerId,
            channel: "vm-response-\(consumerPeerId)"
        )

        // Store pending VM — waiting for cloister + provisioning data
        pendingVMs[vmId] = PendingVM(
            vmId: vmId,
            consumerPeerId: consumerPeerId,
            request: request
        )

        // The consumer will now:
        // 1. Negotiate a cloister with us (we handle via CloisterHandler)
        // 2. Send VMProvisioningData over the cloister
        // We listen for provisioning data on "vm-provision-{vmId}"
    }

    func handleProvisioningData(
        _ data: VMProvisioningData,
        from consumerPeerId: PeerId
    ) async throws {
        guard let pending = pendingVMs.removeValue(forKey: data.vmId) else {
            throw ProviderError.unknownVM(data.vmId)
        }

        // Launch the VM with its own identity, bootstrapping to the consumer
        let result = try await vmManager.startVM(
            vmId: data.vmId,
            requirements: pending.request.resources,
            sshPublicKey: pending.request.sshPublicKeys.first ?? "",
            sshUser: pending.request.sshUser,
            meshConfig: MeshVMCloudInitConfig(
                cloisterNetworkKey: data.cloisterNetworkKey,
                bootstrapPeers: [consumerEndpoint],  // consumer is bootstrap
                vmIdentity: data.vmIdentity,
                subnet: data.subnet
            )
        )

        // Send ack over cloister
        try await sendProvisionAck(vmId: data.vmId, to: consumerPeerId)
    }
}
```

### Files to Modify

| File | Changes |
|------|---------|
| `Sources/OmertaConsumer/MeshConsumerClient.swift` | Multi-part request: request → accept → cloister → generate VM identity → provision → ping |
| `Sources/OmertaProvider/MeshProviderDaemon.swift` | Handle multi-part: accept/reject → cloister → receive provisioning → launch |

### Unit Tests

| Test | Description |
|------|-------------|
| `testRequestSentOnMainMesh` | Verify initial request uses main mesh channel |
| `testRequestContainsNoSensitiveData` | Verify VMRequest has no identity, subnet, or keys |
| `testAcceptanceOnMainMesh` | Verify acceptance response on main mesh |
| `testRejectionOnMainMesh` | Verify rejection when at capacity |
| `testRejectionContainsReason` | Verify rejection message includes a reason string |
| `testCloisterCreatedAfterAcceptance` | Verify cloister negotiated only after acceptance |
| `testConsumerGeneratesVMIdentity` | Verify consumer generates a new IdentityKeypair for the VM |
| `testVMIdentityIsDistinctFromConsumer` | Verify VM PeerId differs from consumer PeerId |
| `testEachVMGetsUniqueIdentity` | Request two VMs, verify distinct PeerIds |
| `testProvisioningDataSentOverCloister` | Verify identity + subnet sent over cloister only |
| `testProvisioningContainsCloisterKey` | Verify cloister network key included |
| `testProviderDoesNotGenerateIdentity` | Verify no keypair generation on provider side |
| `testConsumerDetectsVMViaPing` | Consumer pings VM's PeerId after bootstrap |
| `testNoProvisioningWithoutAcceptance` | Consumer does not create cloister if rejected |
| `testVMBootstrapsToConsumer` | VM's bootstrap peer is the consumer, not the provider |
| `testAcceptanceTimeout` | Provider doesn't respond, verify consumer times out |
| `testProvisionAckTimeout` | Provider accepts but doesn't ack provisioning, verify timeout |
| `testMeshJoinTimeout` | VM never bootstraps, verify consumer times out after polling |
| `testProvisioningForWrongVmId` | Provider receives provisioning with unknown vmId, verify error |
| `testProviderCleansUpPendingOnTimeout` | Pending VM not provisioned within timeout, verify provider cleans up |
| `testCloisterNegotiationFailure` | Cloister negotiation fails, verify consumer surfaces error |
| `testDuplicateRequestId` | Two requests with same requestId, verify provider handles correctly |
| `testVMRequestEncodesAsValidJSON` | Verify VMRequest/VMAcceptance/VMProvisioningData round-trip through Codable |

### Integration Tests

| Test | Description |
|------|-------------|
| `testFullRequestToBootstrapFlow` | Consumer → request → accept → cloister → provision → VM boots → ping succeeds |
| `testRejectionDoesNotCreateCloister` | Full flow with rejection, verify no cloister negotiation occurs |
| `testMultipleVMsSequential` | Request 3 VMs sequentially, verify all get distinct identities and bootstrap |
| `testProviderRestartsAfterAcceptance` | Provider crashes between acceptance and provisioning, verify consumer handles gracefully |
| `testConsumerDisconnectsDuringProvisioning` | Consumer loses mesh connectivity mid-flow, verify provider cleans up pending state |

---

## Phase 3: Cloud-Init and Binary Upload

**Goal:** Use cloud-init on stock Ubuntu cloud images to configure omertad.
The provider uploads the omertad binary directly to the VM disk — no
internet access required. Cloud-init handles first-boot configuration:
writing the VM's identity, mesh config, systemd unit, and SSH keys.

### What the Provider Does Before VM Boot

1. Creates an overlay disk from the stock Ubuntu base image
2. Writes the omertad binary to the overlay (mounted or via `virt-customize`)
3. Generates a cloud-init seed ISO containing:
   - VM's `IdentityKeypair` (generated by consumer, received over cloister)
   - Mesh config with cloister network key, consumer as bootstrap peer,
     and consumer-provided subnet
   - systemd unit for omertad
   - SSH authorized keys (received in the initial request on main mesh)
4. Boots the VM with the overlay disk + seed ISO attached

The omertad binary is pre-cached on the provider host.

### Cloud-Init User-Data

```yaml
#cloud-config
packages:
  - openssh-server

write_files:
  - path: /etc/omerta/mesh.json
    content: |
      {
        "networkKey": "<cloister-network-key>",
        "bootstrapPeers": ["<consumer-peer-id>@<consumer-endpoint>"],
        "enableVPN": true,
        "subnet": "<consumer-provided-subnet>"
      }
    permissions: '0600'

  - path: /etc/omerta/identity.json
    content: |
      {
        "peerId": "<vm-peer-id>",
        "publicKey": "<vm-public-key-base64>",
        "privateKey": "<vm-private-key-base64>"
      }
    permissions: '0600'

  - path: /etc/systemd/system/omertad.service
    content: |
      [Unit]
      Description=Omerta Mesh Daemon
      After=network-online.target
      Wants=network-online.target

      [Service]
      Type=simple
      ExecStart=/usr/local/bin/omertad start \
          --config /etc/omerta/mesh.json \
          --vpn
      Restart=always
      RestartSec=5
      Environment=OMERTA_LOG_LEVEL=info

      [Install]
      WantedBy=multi-user.target

runcmd:
  - systemctl enable --now omertad.service
```

Note: `/usr/local/bin/omertad` is already on disk — the provider wrote it
to the overlay image before boot. No download step in cloud-init.

### Extended CloudInitGenerator API

```swift
extension CloudInitGenerator {
    /// Configuration for a VM that joins the cloister network as its own peer.
    public struct MeshVMConfig {
        /// Cloister network key the VM will join
        public let cloisterNetworkKey: Data

        /// Bootstrap peer (the consumer's endpoint)
        public let bootstrapPeers: [String]

        /// VM's own identity (generated by the consumer)
        public let vmIdentity: IdentityKeypair

        /// Consumer-provided subnet (e.g. "10.0.1.0/24")
        public let subnet: String

        /// SSH public keys to install
        public let sshAuthorizedKeys: [String]

        /// SSH user name
        public let sshUser: String
    }

    /// Generate cloud-init user-data for a VM joining the cloister as its own peer.
    ///
    /// The omertad binary is pre-installed on the disk image by the provider.
    /// Cloud-init only handles configuration:
    /// 1. Write mesh config with cloister key and consumer as bootstrap
    /// 2. Write VM's own identity
    /// 3. Install and start the omertad systemd service
    /// 4. Configure SSH with the provided authorized keys
    public static func generateMeshUserData(_ config: MeshVMConfig) throws -> String
}
```

### Files to Create/Modify

| File | Changes |
|------|---------|
| `Sources/OmertaVM/CloudInitGenerator.swift` | Add `MeshVMConfig` and `generateMeshUserData()` |
| `Sources/OmertaVM/VMManager.swift` | Write omertad binary to overlay disk before boot |
| `Tests/OmertaVMTests/CloudInitGeneratorTests.swift` | Tests for mesh cloud-init generation |

### Unit Tests

| Test | Description |
|------|-------------|
| `testMeshCloudInitGeneration` | Generate cloud-init, verify YAML is valid |
| `testCloudInitContainsVMIdentity` | Verify VM's own identity is included |
| `testCloudInitIdentityIsNotConsumers` | Verify identity in cloud-init is the VM's, not the consumer's |
| `testCloudInitContainsCloisterKey` | Verify cloister network key is included |
| `testCloudInitConsumerAsBootstrap` | Verify consumer is the bootstrap peer |
| `testCloudInitContainsSubnet` | Verify consumer-provided subnet is included |
| `testCloudInitNoDownloadStep` | Verify runcmd does NOT download omertad |
| `testCloudInitContainsSystemdUnit` | Verify omertad.service is written |
| `testCloudInitSystemdUnitAfterNetwork` | Verify systemd unit has After=network-online.target |
| `testCloudInitSystemdUnitRestartsOnFailure` | Verify Restart=always in systemd unit |
| `testCloudInitFilePermissions` | Verify mesh.json and identity.json have 0600 permissions |
| `testCloudInitContainsSSHKeys` | Verify SSH authorized keys are present |
| `testCloudInitSSHKeyBasedAuthOnly` | Verify password auth is disabled |
| `testOmertadBinaryWrittenToOverlay` | Verify binary exists on disk image before boot |
| `testOmertadBinaryIsExecutable` | Verify binary has executable permissions on overlay |
| `testOverlayDiskCreatedFromBase` | Verify overlay is a COW copy, base image is unchanged |
| `testSeedISOContainsUserData` | Verify cloud-init ISO has user-data file |
| `testSeedISOContainsMetaData` | Verify cloud-init ISO has meta-data with instance-id |
| `testSeedISOContainsNetworkConfig` | Verify cloud-init ISO has network-config |
| `testCloudInitNoSensitiveDataInLogs` | Verify private key is not echoed in runcmd or logs |

### Integration Tests

| Test | Description |
|------|-------------|
| `testVMBootsWithCloudInit` | Boot VM with generated cloud-init, verify omertad service starts |
| `testVMOmertadLoadsIdentity` | Boot VM, verify omertad loads the VM identity from /etc/omerta/identity.json |
| `testVMOmertadJoinsCloister` | Boot VM, verify omertad uses cloister network key from mesh.json |
| `testVMSSHAccessible` | Boot VM, verify SSH login works with provided authorized key |

---

## Phase 4: Bootstrap Connectivity and Consumer Gateway Routing

**Goal:** Ensure the VM can reach its bootstrap peer (the consumer, via
the provider host) to join the cloister network. After joining, all
internet-bound traffic routes through the mesh overlay to the consumer,
which is the gateway on the virtual network.

### Traffic Model

```
VM application traffic (internet-bound):
  app → omerta0 (TUN) → mesh → consumer (gateway) → internet

VM mesh bootstrap traffic:
  omertad → eth0 → TAP → provider host → consumer (bootstrap peer)
```

The VM's `eth0` (TAP link to the provider) carries **only** encrypted
mesh UDP. All application traffic — including internet access — goes
through `omerta0` to the consumer, which is the default gateway on the
virtual network.

### Egress Filtering: Protocol and Network ID Check

The VM's `eth0` must only carry valid omerta mesh protocol traffic for
the correct cloister network. Omertad validates outgoing packets at the
protocol level:

- Packets on `eth0` must conform to the **omerta mesh protocol format**
- Packets must carry the correct **cloister network ID**
- Any packet that doesn't match is dropped

This is endpoint-agnostic — the consumer can change endpoints freely
(NAT rebinding, roaming, reconnection) without any filter updates.

```swift
/// Validates that outgoing eth0 traffic conforms to the omerta
/// protocol and uses the expected cloister network ID.
struct VMEgressFilter {
    /// The cloister network ID this VM belongs to
    let cloisterNetworkId: String

    /// Check if an outgoing packet is allowed on eth0.
    /// Only omerta protocol packets with the correct network ID pass.
    func isAllowed(_ packet: Data) -> Bool {
        guard isOmertaProtocol(packet) else { return false }
        guard extractNetworkId(packet) == cloisterNetworkId else { return false }
        return true
    }
}
```

### Bootstrap Path

The VM doesn't have direct connectivity to the consumer. The bootstrap
path goes through the provider host:

```
VM omertad → eth0 (TAP) → provider host network → consumer endpoint
```

The provider host forwards mesh UDP from the VM's TAP interface to the
consumer. The provider itself is not on the cloister network — it just
provides network connectivity.

**Linux:**
```bash
# Provider creates a TAP device for the VM
ip tuntap add mode tap vm-tap-${VM_ID}
ip addr add ${TAP_HOST_IP}/30 dev vm-tap-${VM_ID}
ip link set vm-tap-${VM_ID} up

# Enable forwarding so VM can reach the consumer's endpoint
# (the consumer is on the internet, reachable from provider)
iptables -t nat -A POSTROUTING -s ${TAP_SUBNET} -o ${WAN_IFACE} -j MASQUERADE
```

**macOS:**
```swift
// VZNATNetworkDeviceAttachment gives VM a private NAT interface
// VM can reach the consumer's endpoint through the NAT
```

### TAP Subnet Allocation

Each VM gets its own `/30` from a pool managed by the provider. This is
just the point-to-point link between the VM and the provider host — not
the mesh overlay subnet.

```swift
/// Allocates /30 subnets for VM TAP links from a local pool.
struct TAPSubnetAllocator {
    /// Base range: 192.168.100.0/24 → up to 64 VMs
    /// VM 0: host=192.168.100.1/30, guest=192.168.100.2/30
    /// VM 1: host=192.168.100.5/30, guest=192.168.100.6/30
    func allocate(vmIndex: Int) -> (hostIP: String, guestIP: String)
    func release(vmIndex: Int)
}
```

### Cloud-Init Network Config

The TAP guest IP is injected via cloud-init network-config:

```yaml
network:
  version: 2
  ethernets:
    eth0:
      addresses:
        - 192.168.100.2/30
      routes:
        - to: default
          via: 192.168.100.1
```

### Post-Join Routing

Once omertad joins the cloister network and creates the TUN interface:

```
omerta0 (consumer-provided subnet) — all application traffic
  - Default route: 0.0.0.0/0 → omerta0 (consumer is gateway)
  - Mesh subnet routes → omerta0
  - Internet traffic → omerta0 → mesh → consumer → internet

eth0 (provider-allocated TAP /30) — mesh UDP only
  - Route to provider TAP host IP → eth0
  - Egress filter: only omerta protocol with correct cloister network ID
```

### Files to Create

| File | Description |
|------|-------------|
| `Sources/OmertaVM/Platform/TAPSubnetAllocator.swift` | Per-VM /30 allocation for TAP links |
| `Sources/OmertaVM/VMEgressFilter.swift` | Protocol-level egress filter: omerta protocol + cloister network ID |

### Files to Modify

| File | Changes |
|------|---------|
| `Sources/OmertaVM/Platform/LinuxVMPlatform.swift` | Use TAPSubnetAllocator, enable NAT for consumer reachability |

### Unit Tests

| Test | Description |
|------|-------------|
| `testTAPSubnetAllocation` | Verify unique /30 per VM, no collisions |
| `testTAPSubnetAllocationExhaustion` | Allocate all 64 slots, verify error on 65th |
| `testTAPSubnetRelease` | Verify released subnets can be reused |
| `testTAPSubnetReleaseAndReallocate` | Release slot 3, allocate again, verify slot 3 reused |
| `testTAPDeviceCreated` | Verify TAP device exists after VM create |
| `testTAPDeviceCleanedUp` | Verify TAP device removed after VM destroy |
| `testTAPDeviceNameUnique` | Each VM gets a distinct TAP device name |
| `testDefaultRouteIsOmerta0` | Verify 0.0.0.0/0 routes through omerta0 (consumer gateway) |
| `testEgressFilterAllowsOmertaProtocol` | Valid omerta packet with correct network ID passes |
| `testEgressFilterBlocksNonOmerta` | Non-omerta packet on eth0 is dropped |
| `testEgressFilterBlocksWrongNetworkId` | Omerta packet with wrong cloister network ID is dropped |
| `testEgressFilterBlocksEmptyPacket` | Empty/truncated packet is dropped |
| `testEgressFilterBlocksMalformedOmerta` | Packet with omerta magic bytes but invalid structure is dropped |
| `testEgressFilterAllowsRegardlessOfEndpoint` | Same packet to different destination IPs passes if protocol and network ID match |
| `testNATMasqueradeConfigured` | Verify iptables MASQUERADE rule exists for TAP subnet |
| `testNATMasqueradeCleanedUp` | After VM destroy, verify MASQUERADE rule removed |

### Integration Tests

| Test | Description |
|------|-------------|
| `testVMCanReachConsumerViaProvider` | VM mesh UDP reaches consumer through provider's NAT |
| `testVMCannotReachArbitraryInternet` | VM eth0 cannot reach arbitrary internet hosts directly |
| `testVMInternetViaConsumerGateway` | VM app traffic exits via omerta0 → consumer → internet |
| `testConsumerEndpointChangesVMStillConnected` | Consumer roams to new endpoint, VM maintains mesh connectivity |
| `testVMBootstrapToConsumerFullPath` | VM boots, contacts consumer via provider TAP+NAT, joins cloister, ping succeeds |
| `testMultipleVMsDistinctTAPSubnets` | Launch 3 VMs, verify each has a unique /30 TAP link |
| `testVMDestroyReleasesAllResources` | Destroy VM, verify TAP device, NAT rule, overlay disk, and subnet all cleaned up |

---

## Handoff to VIRTUAL_NETWORK_REWORK Phase 15

After completing phases 1-4, the system is ready for Phase 15 of
VIRTUAL_NETWORK_REWORK.md, which tests:

- VM boots, omertad starts (binary pre-installed), joins cloister network
- VM is its own peer (own PeerId, generated by consumer)
- VM bootstraps to the consumer directly
- Consumer detects VM via `ping(vmPeerId)`
- VM uses consumer-provided subnet
- Internet traffic from VM routes through consumer (gateway)
- SSH to VM over mesh
- VM-to-VM communication
- Consumer with TUN can `ssh omerta@10.0.x.x`

The in-VM daemon model means Phase 15 tests work without any special
provider-side packet routing — the VM is just another peer on the cloister
network, using the consumer as its gateway to the internet.
